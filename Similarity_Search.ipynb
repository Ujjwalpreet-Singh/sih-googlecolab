{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1d2qzF4Wm2Y5H_DR7GHZyqcmm6HBM5L4o",
      "authorship_tag": "ABX9TyMM44vvYg3CJ6DcSkOjU+/b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ujjwalpreet-Singh/sih-googlecolab/blob/main/Similarity_Search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXJUbIy9Giz0"
      },
      "outputs": [],
      "source": [
        "pip install pypdf python-docx pytesseract Pillow google-generativeai ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75d1f8b0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ff2b15e"
      },
      "source": [
        "import os\n",
        "import json # Added import for json\n",
        "\n",
        "# 2. Construct the full paths to the 'processed resumes' and 'processed jobs' subfolders\n",
        "resumes_folder_path = \"/content/drive/MyDrive/SIH-Project/processed_resumes\"\n",
        "jobs_folder_path = \"/content/drive/MyDrive/SIH-Project/processed-jobs\"\n",
        "\n",
        "print(f\"\\nAttempting to access resume folder: {resumes_folder_path}\")\n",
        "print(f\"Attempting to access jobs folder: {jobs_folder_path}\")\n",
        "\n",
        "# 3. List the contents of both subfolders to confirm that files are present\n",
        "print(\"\\nListing contents of 'processed resumes' folder:\")\n",
        "if os.path.exists(resumes_folder_path):\n",
        "    resume_files = [f for f in os.listdir(resumes_folder_path) if f.endswith('.txt') or f.endswith('.pdf') or f.endswith('json')]\n",
        "    if resume_files:\n",
        "        print(resume_files)\n",
        "    else:\n",
        "        print(\"No resume files found or folder is empty.\")\n",
        "else:\n",
        "    print(f\"Error: '{resumes_folder_path}' not found.\")\n",
        "    resume_files = []\n",
        "\n",
        "print(\"\\nListing contents of 'processed jobs' folder:\")\n",
        "if os.path.exists(jobs_folder_path):\n",
        "    job_files = [f for f in os.listdir(jobs_folder_path) if f.endswith('.txt') or f.endswith('.pdf') or f.endswith('json')]\n",
        "    if job_files:\n",
        "        print(job_files)\n",
        "    else:\n",
        "        print(\"No job files found or folder is empty.\")\n",
        "else:\n",
        "    print(f\"Error: '{jobs_folder_path}' not found.\")\n",
        "    job_files = []\n",
        "\n",
        "# 4. From the 'processed resumes' folder, select one resume file and read its content\n",
        "selected_resume_content = \"\"\n",
        "if resume_files:\n",
        "    # Assuming the resume is also a JSON file as per previous execution\n",
        "    selected_resume_file = os.path.join(resumes_folder_path, resume_files[0])\n",
        "    try:\n",
        "        if selected_resume_file.endswith('.json'):\n",
        "            with open(selected_resume_file, 'r', encoding='utf-8') as f:\n",
        "                resume_data = json.load(f)\n",
        "                # Extract relevant text from the resume JSON for embedding\n",
        "                # For now, let's just use the entire JSON string representation if it's complex\n",
        "                # or pick specific fields. For simplicity, we'll convert back to string.\n",
        "                # A more robust solution would be to define which fields are relevant for embedding.\n",
        "                selected_resume_content = json.dumps(resume_data) # Keep as string for embedding\n",
        "        else: # Handle .txt or .pdf files as before\n",
        "            with open(selected_resume_file, 'r', encoding='utf-8') as f:\n",
        "                selected_resume_content = f.read()\n",
        "        print(f\"\\nSuccessfully read resume file: {resume_files[0]}\")\n",
        "        print(\"First few lines of resume content:\")\n",
        "        print('\\n'.join(selected_resume_content.splitlines()[:5]))\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading resume file {selected_resume_file}: {e}\")\n",
        "else:\n",
        "    print(\"\\nNo resume files available to read.\")\n",
        "\n",
        "# 5. From the 'processed jobs' folder, iterate through all files and read their content\n",
        "job_postings_content = []\n",
        "job_postings_raw_data = [] # New list to store parsed job objects\n",
        "if job_files:\n",
        "    for job_file_name in job_files:\n",
        "        job_file_path = os.path.join(jobs_folder_path, job_file_name)\n",
        "        try:\n",
        "            if job_file_name.endswith('.json'):\n",
        "                with open(job_file_path, 'r', encoding='utf-8') as f:\n",
        "                    json_data = json.load(f)\n",
        "                    # Assuming json_data is a list of job dictionaries\n",
        "                    for job_obj in json_data:\n",
        "                        job_text = ''\n",
        "                        # Extract text for embedding - combine relevant field\n",
        "                        if 'skills' in job_obj and isinstance(job_obj['skills'], list):\n",
        "                            job_text += ' ' + ', '.join(job_obj['skills'])\n",
        "                        job_postings_content.append(job_text.strip())\n",
        "                        job_postings_raw_data.append(job_obj) # Store the original job dict\n",
        "            else: # Handle .txt or .pdf files as before\n",
        "                with open(job_file_path, 'r', encoding='utf-8') as f:\n",
        "                    job_postings_content.append(f.read())\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading job file {job_file_path}: {e}\")\n",
        "    print(f\"\\nSuccessfully read {len(job_postings_content)} individual job postings.\")\n",
        "    if job_postings_content:\n",
        "        print(\"First job posting content snippet:\")\n",
        "        print(job_postings_content[0][:200] + '...') # Print first 200 chars\n",
        "else:\n",
        "    print(\"\\nNo job files available to read.\")\n",
        "\n",
        "print(job_postings_content)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9649af61"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "print(job_postings_content)\n",
        "print(selected_resume_content)\n",
        "#\n",
        "# 3. Load a pre-trained Sentence-BERT model\n",
        "print(\"Loading Sentence-BERT model 'all-MiniLM-L6-v2'...\")\n",
        "model = SentenceTransformer('multi-qa-mpnet-base-dot-v1')\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# 4. Generate the embedding for the selected resume content\n",
        "resume_embedding = None\n",
        "if selected_resume_content:\n",
        "    print(\"Generating embedding for the selected resume...\")\n",
        "    resume_embedding = model.encode(selected_resume_content)\n",
        "    print(f\"Resume embedding generated with shape: {resume_embedding.shape}\")\n",
        "else:\n",
        "    print(\"Warning: No resume content found. Please ensure the file reading step was successful (cell ID cde87e8e).\")\n",
        "\n",
        "# 5. Generate embeddings for all job postings\n",
        "job_embeddings = []\n",
        "if job_postings_content:\n",
        "    print(f\"Generating embeddings for {len(job_postings_content)} job postings...\")\n",
        "    job_embeddings = model.encode(job_postings_content)\n",
        "    print(f\"Job embeddings generated with shape: {job_embeddings.shape}\")\n",
        "else:\n",
        "    print(\"Warning: No job postings content found. Please ensure the file reading step was successful (cell ID cde87e8e).\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8e27694"
      },
      "source": [
        "import json\n",
        "\n",
        "# 1. Extract and combine focused content from resume_data\n",
        "focused_resume_content = []\n",
        "\n",
        "# Extract skills\n",
        "if 'skills' in resume_data and isinstance(resume_data['skills'], list):\n",
        "    focused_resume_content.extend(resume_data['skills'])\n",
        "\n",
        "# Extract experience descriptions\n",
        "if 'experience' in resume_data and isinstance(resume_data['experience'], list):\n",
        "    for exp in resume_data['experience']:\n",
        "        if 'description' in exp:\n",
        "            focused_resume_content.append(exp['description'])\n",
        "\n",
        "focused_resume_content_str = ' '.join(focused_resume_content).strip()\n",
        "\n",
        "print(f\"\\nGenerated focused resume content (first 200 chars): {focused_resume_content_str[:200]}...\")\n",
        "\n",
        "# 2. Extract and combine focused content from job_postings_raw_data\n",
        "focused_job_contents = []\n",
        "for job_obj in job_postings_raw_data:\n",
        "    job_focused_text = []\n",
        "    if 'skills' in job_obj and isinstance(job_obj['skills'], list):\n",
        "        job_focused_text.extend(job_obj['skills'])\n",
        "    if 'responsibilities' in job_obj and isinstance(job_obj['responsibilities'], list):\n",
        "        job_focused_text.extend(job_obj['responsibilities'])\n",
        "    focused_job_contents.append(' '.join(job_focused_text).strip())\n",
        "\n",
        "print(f\"Generated {len(focused_job_contents)} focused job contents. First one (first 200 chars): {focused_job_contents[0][:200]}...\")\n",
        "\n",
        "# 3. Generate embedding for focused_resume_content\n",
        "focused_resume_embedding = None\n",
        "if focused_resume_content_str:\n",
        "    print(\"\\nGenerating embedding for focused resume content...\")\n",
        "    focused_resume_embedding = model.encode(focused_resume_content_str)\n",
        "    print(f\"Focused Resume embedding generated with shape: {focused_resume_embedding.shape}\")\n",
        "else:\n",
        "    print(\"Warning: No focused resume content found.\")\n",
        "\n",
        "# 4. Generate embeddings for focused_job_contents\n",
        "focused_job_embeddings = []\n",
        "if focused_job_contents:\n",
        "    print(f\"\\nGenerating embeddings for {len(focused_job_contents)} focused job postings...\")\n",
        "    focused_job_embeddings = model.encode(focused_job_contents)\n",
        "    print(f\"Focused Job embeddings generated with shape: {focused_job_embeddings.shape}\")\n",
        "else:\n",
        "    print(\"Warning: No focused job postings content found.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2453c64b"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Reshape focused_resume_embedding for compatibility with cosine_similarity if it's 1D\n",
        "if focused_resume_embedding.ndim == 1:\n",
        "    focused_resume_embedding_reshaped = focused_resume_embedding.reshape(1, -1)\n",
        "else:\n",
        "    focused_resume_embedding_reshaped = focused_resume_embedding\n",
        "\n",
        "# Calculate cosine similarity\n",
        "focused_similarity_scores = cosine_similarity(focused_resume_embedding_reshaped, focused_job_embeddings)\n",
        "\n",
        "print(\"Cosine Similarity Scores (Focused Resume vs. Focused Job Postings):\")\n",
        "print(focused_similarity_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4678e7b2"
      },
      "source": [
        "num_top_matches = 5 # Display top 5 matches, or fewer if less are available\n",
        "similarity_threshold_low = 0.55 # Define lower bound for 'around 0.5'\n",
        "similarity_threshold_high = 0.95 # Define upper bound for 'around 0.5'\n",
        "\n",
        "# Create a list of (score, job_posting_raw_data) tuples\n",
        "# Ensure job_postings_raw_data and focused_similarity_scores match in length\n",
        "if len(job_postings_raw_data) == focused_similarity_scores.shape[1]:\n",
        "    scored_focused_jobs = []\n",
        "    for i in range(len(job_postings_raw_data)):\n",
        "        scored_focused_jobs.append((focused_similarity_scores[0, i], job_postings_raw_data[i]))\n",
        "\n",
        "    # Sort the jobs by similarity score in descending order\n",
        "    scored_focused_jobs.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "    print(f\"\\nTop {min(num_top_matches, len(scored_focused_jobs))} Matching Job Postings (Focused Embeddings) around {similarity_threshold_low}-{similarity_threshold_high} similarity mark:\\n\")\n",
        "\n",
        "    filtered_jobs_count = 0\n",
        "    for score, job_data in scored_focused_jobs:\n",
        "        if similarity_threshold_low <= score <= similarity_threshold_high:\n",
        "            if filtered_jobs_count < num_top_matches:\n",
        "                filtered_jobs_count += 1\n",
        "                print(f\"--- Rank {filtered_jobs_count} ---\")\n",
        "                print(f\"Similarity Score: {score:.4f}\")\n",
        "                print(f\"Job Title: {job_data.get('job_title', 'N/A')}\")\n",
        "                print(f\"Company: {job_data.get('company_name', 'N/A')}\")\n",
        "\n",
        "                # Display skills if available\n",
        "                skills = job_data.get('skills', 'N/A')\n",
        "                if isinstance(skills, list):\n",
        "                    print(f\"Skills: {', '.join(skills)}\")\n",
        "                else:\n",
        "                    print(f\"Skills: {skills}\")\n",
        "\n",
        "                # Display a snippet of the original description\n",
        "                description_snippet = job_data.get('original_description', 'N/A')\n",
        "                if len(description_snippet) > 200:\n",
        "                    description_snippet = description_snippet[:200] + '...'\n",
        "                print(f\"Description Snippet: {description_snippet}\")\n",
        "                print(\"---------------------\\n\")\n",
        "            else:\n",
        "                break # Stop if we have found 5 matching jobs\n",
        "\n",
        "    if filtered_jobs_count == 0:\n",
        "        print(\"No job postings found within the specified similarity range.\")\n",
        "\n",
        "else:\n",
        "    print(\"Error: Mismatch between number of job postings and focused similarity scores.\")\n",
        "    print(f\"Jobs found: {len(job_postings_raw_data)}, Scores found: {focused_similarity_scores.shape[1]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57e948a5"
      },
      "source": [
        "all_unique_skills = set()\n",
        "\n",
        "# 2. Extract skills from resume_data\n",
        "if 'skills' in resume_data and isinstance(resume_data['skills'], list):\n",
        "    all_unique_skills.update(resume_data['skills'])\n",
        "    print(f\"Added {len(resume_data['skills'])} skills from resume_data.\")\n",
        "else:\n",
        "    print(\"No skills found in resume_data or skills not in list format.\")\n",
        "\n",
        "# 3. Iterate through job_postings_raw_data and extract skills\n",
        "job_skills_count = 0\n",
        "for job_obj in job_postings_raw_data:\n",
        "    if 'skills' in job_obj and isinstance(job_obj['skills'], list):\n",
        "        all_unique_skills.update(job_obj['skills'])\n",
        "        job_skills_count += len(job_obj['skills'])\n",
        "print(f\"Added {job_skills_count} skills (including duplicates) from all job postings.\")\n",
        "\n",
        "# 4. Convert the set to a list and print summary\n",
        "all_unique_skills_list = list(all_unique_skills)\n",
        "print(f\"\\nTotal number of unique skills collected: {len(all_unique_skills_list)}\")\n",
        "print(f\"First 10 unique skills: {all_unique_skills_list[:10]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce760f83"
      },
      "source": [
        "print(f\"Generating embeddings for {len(all_unique_skills_list)} unique skills...\")\n",
        "skill_embeddings = model.encode(all_unique_skills_list, show_progress_bar=True)\n",
        "print(f\"Skill embeddings generated with shape: {skill_embeddings.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "347bd2d2"
      },
      "source": [
        "applicant_skills_for_embedding = []\n",
        "if 'skills' in resume_data and isinstance(resume_data['skills'], list):\n",
        "    applicant_skills_for_embedding = resume_data['skills']\n",
        "\n",
        "print(f\"Generating embeddings for {len(applicant_skills_for_embedding)} applicant skills...\")\n",
        "applicant_skill_embeddings = model.encode(applicant_skills_for_embedding, show_progress_bar=True)\n",
        "print(f\"Applicant skill embeddings generated with shape: {applicant_skill_embeddings.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c06ed71"
      },
      "source": [
        "semantic_similarity_threshold = 0.5 # Define a semantic similarity threshold for individual skills\n",
        "\n",
        "print(f\"\\nIdentifying semantically missing skills for relevant job postings (semantic similarity threshold: {semantic_similarity_threshold}):\\n\")\n",
        "\n",
        "filtered_jobs_count = 0\n",
        "for score, job_data in scored_focused_jobs:\n",
        "    if 0.55 <= score <= 0.70:\n",
        "        if filtered_jobs_count < num_top_matches:\n",
        "            filtered_jobs_count += 1\n",
        "            print(f\"--- Rank {filtered_jobs_count} ---\")\n",
        "            print(f\"Overall Similarity Score: {score:.4f}\")\n",
        "            print(f\"Job Title: {job_data.get('job_title', 'N/A')}\")\n",
        "\n",
        "            job_skills_list = job_data.get('skills', [])\n",
        "            semantically_missing_skills = []\n",
        "\n",
        "            if isinstance(job_skills_list, list) and job_skills_list:\n",
        "                for job_skill in job_skills_list:\n",
        "                    # Get the embedding for the current job skill\n",
        "                    try:\n",
        "                        job_skill_index = all_unique_skills_list.index(job_skill)\n",
        "                        job_skill_embedding = skill_embeddings[job_skill_index]\n",
        "\n",
        "                        # Calculate cosine similarity between job_skill_embedding and all applicant_skill_embeddings\n",
        "                        if applicant_skill_embeddings.size > 0: # Ensure there are applicant skills to compare against\n",
        "                            # Reshape for cosine_similarity function if needed\n",
        "                            job_skill_embedding_reshaped = job_skill_embedding.reshape(1, -1)\n",
        "                            individual_skill_similarities = cosine_similarity(job_skill_embedding_reshaped, applicant_skill_embeddings)\n",
        "\n",
        "                            # Find the maximum similarity score for this job skill against any applicant skill\n",
        "                            max_similarity_to_applicant_skills = np.max(individual_skill_similarities)\n",
        "\n",
        "                            # If the maximum similarity is below the semantic threshold, consider it missing\n",
        "                            if max_similarity_to_applicant_skills < semantic_similarity_threshold:\n",
        "                                semantically_missing_skills.append(job_skill)\n",
        "                        else:\n",
        "                            # If no applicant skills, all job skills are effectively 'missing'\n",
        "                            semantically_missing_skills.append(job_skill)\n",
        "\n",
        "                    except ValueError:\n",
        "                        # This should ideally not happen if all job skills were added to all_unique_skills_list\n",
        "                        # but as a fallback, if a skill isn't in our embedded list, it's 'missing'\n",
        "                        semantically_missing_skills.append(job_skill)\n",
        "\n",
        "                if semantically_missing_skills:\n",
        "                    print(f\"Semantically missing skills for this role: {', '.join(semantically_missing_skills)}\")\n",
        "                else:\n",
        "                    print(\"No semantically missing skills identified for this role.\")\n",
        "            else:\n",
        "                print(\"No skills listed for this job posting or format is incorrect.\")\n",
        "            print(\"---------------------\\n\")\n",
        "        else:\n",
        "            break\n",
        "\n",
        "if filtered_jobs_count == 0:\n",
        "    print(\"No job postings found within the specified overall similarity range (0.45-0.55).\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content')\n",
        "from resume_parser import process\n",
        "process(\"resume.txt\")"
      ],
      "metadata": {
        "id": "EFpqTqJDVG8m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}