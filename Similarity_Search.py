import os
import json
from sentence_transformers import SentenceTransformer
import numpy as np
from fastapi.responses import PlainTextResponse


print("Loading Sentence-BERT model 'multi-qa-mpnet-base-dot-v1'...")
model = SentenceTransformer('multi-qa-mpnet-base-dot-v1')
print("Model loaded successfully.")

def get_files(resumes_folder_path,jobs_folder_path):
    print(f"\nAttempting to access resume folder: {resumes_folder_path}")
    print(f"Attempting to access jobs folder: {jobs_folder_path}")

    # 3. List the contents of both subfolders to confirm that files are present
    print("\nListing contents of 'processed resumes' folder:")
    if os.path.exists(resumes_folder_path):
        resume_files = [f for f in os.listdir(resumes_folder_path) if
                        f.endswith('.txt') or f.endswith('.pdf') or f.endswith('json')]
        if resume_files:
            print(resume_files)
        else:
            print("No resume files found or folder is empty.")
    else:
        print(f"Error: '{resumes_folder_path}' not found.")
        resume_files = []

    print("\nListing contents of 'processed jobs' folder:")
    if os.path.exists(jobs_folder_path):
        job_files = [f for f in os.listdir(jobs_folder_path) if
                     f.endswith('.txt') or f.endswith('.pdf') or f.endswith('json')]
        if job_files:
            print(job_files)
        else:
            print("No job files found or folder is empty.")
    else:
        print(f"Error: '{jobs_folder_path}' not found.")
        job_files = []


    return resume_files, job_files

def load_resume_data(resumes_folder_path,resume_files):
    selected_resume_content = ""
    if resume_files:
        # Assuming the resume is also a JSON file as per previous execution
        selected_resume_file = os.path.join(resumes_folder_path, resume_files[0])
        try:
            if selected_resume_file.endswith('.json'):
                with open(selected_resume_file, 'r', encoding='utf-8') as f:
                    resume_data = json.load(f)
                    # Extract relevant text from the resume JSON for embedding
                    # For now, let's just use the entire JSON string representation if it's complex
                    # or pick specific fields. For simplicity, we'll convert back to string.
                    # A more robust solution would be to define which fields are relevant for embedding.
                    selected_resume_content = json.dumps(resume_data)  # Keep as string for embedding
            else:  # Handle .txt or .pdf files as before
                with open(selected_resume_file, 'r', encoding='utf-8') as f:
                    selected_resume_content = f.read()
            print(f"\nSuccessfully read resume file: {resume_files[0]}")
        except Exception as e:
            print(f"Error reading resume file {selected_resume_file}: {e}")
    else:
        print("\nNo resume files available to read.")

    return selected_resume_content,resume_data

def load_job_data(jobs_folder_path,job_files):
    job_postings_content = []
    job_postings_raw_data = []  # New list to store parsed job objects
    if job_files:
        for job_file_name in job_files:
            job_file_path = os.path.join(jobs_folder_path, job_file_name)
            try:
                if job_file_name.endswith('.json'):
                    with open(job_file_path, 'r', encoding='utf-8') as f:
                        json_data = json.load(f)
                        # Assuming json_data is a list of job dictionaries
                        for job_obj in json_data:
                            job_text = ''
                            # Extract text for embedding - combine relevant field
                            if 'skills' in job_obj and isinstance(job_obj['skills'], list):
                                job_text += ' ' + ', '.join(job_obj['skills'])
                            job_postings_content.append(job_text.strip())
                            job_postings_raw_data.append(job_obj)  # Store the original job dict
                else:  # Handle .txt or .pdf files as before
                    with open(job_file_path, 'r', encoding='utf-8') as f:
                        job_postings_content.append(f.read())
            except Exception as e:
                print(f"Error reading job file {job_file_path}: {e}")
        print(f"\nSuccessfully read {len(job_postings_content)} individual job postings.")
        if job_postings_content:
            print("First job posting content snippet:")
            print(job_postings_content[0][:200] + '...')
            # Print first 200 chars
    else:
        print("\nNo job files available to read.")

    return job_postings_content,job_postings_raw_data


def gettop5(selected_resume_content, resume_data, job_postings_content, job_postings_raw_data, context):

    import numpy as np
    from sklearn.metrics.pairwise import cosine_similarity

    print("\n========== STARTING SEMANTIC JOB MATCHING ==========\n")

    # =========================================================
    # 1Ô∏è‚É£ FULL RESUME EMBEDDING
    # =========================================================
    resume_embedding = None
    if selected_resume_content:
        print("Generating embedding for full resume...")
        resume_embedding = model.encode(selected_resume_content)
        print(f"Resume embedding shape: {resume_embedding.shape}")
    else:
        print("‚ö†Ô∏è Warning: No resume content found.")

    # =========================================================
    # 2Ô∏è‚É£ FULL JOB EMBEDDINGS
    # =========================================================
    job_embeddings = None
    if job_postings_content:
        print(f"Generating embeddings for {len(job_postings_content)} full job postings...")
        job_embeddings = model.encode(job_postings_content)
        print(f"Job embeddings shape: {job_embeddings.shape}")
    else:
        print("‚ö†Ô∏è Warning: No job postings content found.")

    # =========================================================
    # 3Ô∏è‚É£ FOCUSED SKILLS + EXPERIENCE (RESUME)
    # =========================================================
    focused_resume_content = []

    if isinstance(resume_data.get('skills'), list):
        focused_resume_content.extend(resume_data['skills'])

    if isinstance(resume_data.get('experience'), list):
        for exp in resume_data['experience']:
            if isinstance(exp, dict) and 'description' in exp:
                focused_resume_content.append(exp['description'])

    focused_resume_content_str = " ".join(focused_resume_content).strip()
    print(f"\nFocused Resume Text (first 200 chars): {focused_resume_content_str[:200]}...")

    if not focused_resume_content_str:
        print("‚ùå ERROR: No focused resume data found. Aborting.")
        return []

    focused_resume_embedding = model.encode(focused_resume_content_str)

    # =========================================================
    # 4Ô∏è‚É£ FOCUSED SKILLS + RESPONSIBILITIES (JOBS)
    # =========================================================
    focused_job_contents = []
    job_responsibilities_only = []

    for job in job_postings_raw_data:
        job_skill_resp = []

        if isinstance(job.get('skills'), list):
            job_skill_resp.extend(job['skills'])

        if isinstance(job.get('responsibilities'), list):
            job_skill_resp.extend(job['responsibilities'])
            job_responsibilities_only.append(" ".join(job['responsibilities']))
        else:
            job_responsibilities_only.append("")

        focused_job_contents.append(" ".join(job_skill_resp))

    focused_job_embeddings = model.encode(focused_job_contents)

    # =========================================================
    # 5Ô∏è‚É£ SKILL COSINE SIMILARITY
    # =========================================================
    focused_similarity_scores = cosine_similarity(
        focused_resume_embedding.reshape(1, -1),
        focused_job_embeddings
    )

    # =========================================================
    # 6Ô∏è‚É£ EXPERIENCE ‚Üî RESPONSIBILITY COSINE SIMILARITY
    # =========================================================
    resume_experience_text = " ".join([
        exp['description'] for exp in resume_data.get('experience', [])
        if isinstance(exp, dict) and 'description' in exp
    ]).strip()

    if resume_experience_text:
        resume_experience_embedding = model.encode(resume_experience_text)
        job_responsibilities_embeddings = model.encode(job_responsibilities_only)

        experience_scores = cosine_similarity(
            resume_experience_embedding.reshape(1, -1),
            job_responsibilities_embeddings
        )[0]
    else:
        experience_scores = np.zeros(len(job_postings_raw_data))

    # =========================================================
    # 7Ô∏è‚É£ FINAL WEIGHTED SCORE (OLD VARIABLES RESTORED)
    # =========================================================
    scored_focused_jobs = []
    num_top_matches = 5

    for i in range(len(job_postings_raw_data)):
        skill_score = focused_similarity_scores[0, i]
        exp_score = experience_scores[i]
        print('skill_score:', skill_score)
        print('exp_score:', exp_score)
        final_score = skill_score + (0.1 if exp_score > 0.6 else 0.0)

        scored_focused_jobs.append((final_score, job_postings_raw_data[i]))

    scored_focused_jobs.sort(key=lambda x: x[0], reverse=True)
    top5 = scored_focused_jobs[:num_top_matches]

    # =========================================================
    # 8Ô∏è‚É£ DISPLAY TOP 5
    # =========================================================
    print("\n========== üéØ FINAL TOP 5 JOB MATCHES ==========\n")

    for idx, (score, job) in enumerate(top5, start=1):
        print(f"--- Rank {idx} ---")
        print(f"‚úÖ Final Score: {score:.4f}")
        print(f"Job Title: {job.get('job_title', 'N/A')}")
        print(f"Company: {job.get('company_name', 'N/A')}")

        skills = job.get("skills", [])
        if isinstance(skills, list):
            print(f"Skills: {', '.join(skills)}")

        desc = job.get("original_description", "N/A")
        print(f"Description Snippet: {desc[:200]}...")
        print("----------------------------\n")

    results_json = {
        "title": "FINAL TOP 5 JOB MATCHES",
        "total_matches": len(top5),
        "matches": []
    }

    for idx, (score, job) in enumerate(top5, start=1):
        skills = job.get("skills", [])
        desc = job.get("original_description", "N/A")

        results_json["matches"].append({
            "rank": idx,
            "final_score": round(float(score), 4),  # ‚úÖ NumPy-safe
            "job_title": job.get("job_title", "N/A"),
            "company": job.get("company_name", "N/A"),
            "skills": skills if isinstance(skills, list) else [],
            "description_snippet": desc[:200]
        })

    return results_json
    # =========================================================
    # ‚úÖ CONTEXT BLOCK (UNCHANGED BUT NOW WORKS)
    # =========================================================
    if True == context:

        all_unique_skills = set()

        if 'skills' in resume_data and isinstance(resume_data['skills'], list):
            all_unique_skills.update(resume_data['skills'])
            print(f"Added {len(resume_data['skills'])} skills from resume_data.")

        job_skills_count = 0
        for job_obj in job_postings_raw_data:
            if 'skills' in job_obj and isinstance(job_obj['skills'], list):
                all_unique_skills.update(job_obj['skills'])
                job_skills_count += len(job_obj['skills'])

        print(f"Added {job_skills_count} skills (including duplicates) from job postings.")

        all_unique_skills_list = list(all_unique_skills)

        print(f"\nTotal unique skills: {len(all_unique_skills_list)}")
        print(f"First 10 skills: {all_unique_skills_list[:10]}")

        skill_embeddings = model.encode(all_unique_skills_list, show_progress_bar=True)

        applicant_skills_for_embedding = resume_data.get("skills", [])
        applicant_skill_embeddings = model.encode(applicant_skills_for_embedding, show_progress_bar=True)

        semantic_similarity_threshold = 0.5

        print(f"\nIdentifying semantically missing skills:\n")

        filtered_jobs_count = 0
        for score, job_data in scored_focused_jobs:
            if 0.55 <= score <= 0.70 and filtered_jobs_count < num_top_matches:
                filtered_jobs_count += 1

                print(f"--- Rank {filtered_jobs_count} ---")
                print(f"Overall Score: {score:.4f}")
                print(f"Job Title: {job_data.get('job_title', 'N/A')}")

                job_skills_list = job_data.get('skills', [])
                semantically_missing_skills = []

                for job_skill in job_skills_list:
                    if job_skill in all_unique_skills_list:
                        job_skill_index = all_unique_skills_list.index(job_skill)
                        job_skill_embedding = skill_embeddings[job_skill_index].reshape(1, -1)

                        similarities = cosine_similarity(job_skill_embedding, applicant_skill_embeddings)
                        if np.max(similarities) < semantic_similarity_threshold:
                            semantically_missing_skills.append(job_skill)

                if semantically_missing_skills:
                    print(f"Missing Skills: {', '.join(semantically_missing_skills)}")
                else:
                    print("No missing skills detected.")

                print("---------------------\n")

    return {
        "status": "success",
        "total_results": len(final_results),
        "top_5_matches": final_results
    }




if __name__ == '__main__':
    resumes_file_path = 'processed_resumes'
    jobs_file_path = 'processed_jobs'

    resume_files, job_files = get_files(resumes_file_path,jobs_file_path)

    selected_resume_content,resume_data = load_resume_data(resumes_file_path,resume_files)

    job_posting_content,job_posting_raw = load_job_data(jobs_file_path,job_files)

    gettop5(selected_resume_content,resume_data,job_posting_content,job_posting_raw,True)
